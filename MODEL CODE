SWIN TRANSFORM MODEL CODE
import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torchvision.models import swin_t, Swin_T_Weights
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm

# Set reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Dataset path
dataset_path = "C:/Users/SANGEETHA S/Downloads/maths_project_dataset"

# Transforms with stronger augmentations
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(0.4, 0.4, 0.4, 0.2),
    transforms.RandomGrayscale(p=0.1),
    transforms.RandomErasing(p=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

val_test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# Load dataset
full_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)
class_names = full_dataset.classes
num_classes = len(class_names)

# Split dataset
train_size = int(0.7 * len(full_dataset))
val_size = int(0.15 * len(full_dataset))
test_size = len(full_dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])
val_dataset.dataset.transform = val_test_transform
test_dataset.dataset.transform = val_test_transform

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)

# Load model
weights = Swin_T_Weights.DEFAULT
model = swin_t(weights=weights)
for param in model.parameters():
    param.requires_grad = False  # Freeze all layers initially
model.head = nn.Linear(model.head.in_features, num_classes)
model = model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)

# Training with unfreeze after few epochs
num_epochs = 30
early_stopping_patience = 6
best_val_acc = 0.0
epochs_no_improve = 0
unfreeze_epoch = 5

for epoch in range(num_epochs):
    if epoch == unfreeze_epoch:
        for param in model.parameters():
            param.requires_grad = True  # Unfreeze for fine-tuning
        print("🔓 All layers unfrozen for fine-tuning.")

    model.train()
    train_preds, train_labels, train_losses = [], [], []

    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1} - Training"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_losses.append(loss.item())
        train_preds += outputs.argmax(1).cpu().tolist()
        train_labels += labels.cpu().tolist()

    train_acc = accuracy_score(train_labels, train_preds)
    avg_train_loss = np.mean(train_losses)

    # Validation
    model.eval()
    val_preds, val_labels, val_losses = [], [], []
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_losses.append(loss.item())
            val_preds += outputs.argmax(1).cpu().tolist()
            val_labels += labels.cpu().tolist()

    val_acc = accuracy_score(val_labels, val_preds)
    avg_val_loss = np.mean(val_losses)
    scheduler.step(val_acc)

    print(f"\n📘 Epoch {epoch+1} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), "best_swin_model.pt")
        epochs_no_improve = 0
        print("✅ Best model saved.")
    else:
        epochs_no_improve += 1
        print(f"⚠ No improvement for {epochs_no_improve} epoch(s).")
        if epochs_no_improve >= early_stopping_patience:
            print("⛔ Early stopping triggered.")
            break

# Load best model and evaluate
model.load_state_dict(torch.load("best_swin_model.pt"))
model.eval()
test_preds, test_labels = [], []
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        test_preds += outputs.argmax(1).cpu().tolist()
        test_labels += labels.cpu().tolist()

test_acc = accuracy_score(test_labels, test_preds)
print(f"\n🏁 Final Test Accuracy: {test_acc:.4f}")
print(f"🏆 Best Validation Accuracy: {best_val_acc:.4f}")

cm = confusion_matrix(test_labels, test_preds)
report = classification_report(test_labels, test_preds, target_names=class_names)

print("\n🧾 Classification Report:")
print(report)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()
